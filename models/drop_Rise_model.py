import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os

# =====================================================
# MODELLER (XGBoost EKLENDÄ°)
# =====================================================
try:
    from xgboost import XGBClassifier
except ImportError:
    print("XGBoost kÃ¼tÃ¼phanesi eksik. LÃ¼tfen 'pip install xgboost' Ã§alÄ±ÅŸtÄ±rÄ±n.")
    # Fallback (Hata vermemesi iÃ§in dummy bir class, ama kurulu varsayÄ±yoruz)
    from sklearn.ensemble import GradientBoostingClassifier as XGBClassifier

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier

# Metrikler ve AraÃ§lar
from sklearn.metrics import (
    classification_report, confusion_matrix, roc_auc_score,
    precision_score, recall_score, f1_score, precision_recall_curve
)
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
import warnings

warnings.filterwarnings("ignore")


# =====================================================
# CONFIGURATION
# =====================================================
class Config:
    CSV_PATH = "izsu_health_factor.csv"
    TARGET_COL = "HealthFactor"
    DROP_THRESHOLD = -0.05
    RISE_THRESHOLD = 0.05
    OUTPUT_DIR = "ai_outputs_champion_models"
    TARGET_RECALL = 0.80  # Hedeflenen yakalama oranÄ± (%80)
    RANDOM_STATE = 42


# =====================================================
# YARDIMCI FONKSÄ°YON: DETAYLI METRÄ°K ANLATIMI (EÄÄ°TMEN MODU)
# =====================================================
def explain_metrics(y_true, y_pred, y_probs, label_name="DÃœÅÃœÅ"):
    """
    Bu fonksiyon standart metrikleri hesaplar ve kullanÄ±cÄ±ya
    ne anlama geldiklerini terminalde ders verir gibi anlatÄ±r.
    """
    recall = recall_score(y_true, y_pred, zero_division=0)
    precision = precision_score(y_true, y_pred, zero_division=0)
    f1 = f1_score(y_true, y_pred, zero_division=0)
    support_positive = sum(y_true)

    try:
        auc = roc_auc_score(y_true, y_probs)
    except:
        auc = 0.5

    print(f"\n{'=' * 20} {label_name} Ä°Ã‡Ä°N DETAYLI ANALÄ°Z RAPORU {'=' * 20}")
    print(f"Classification Report:\n{classification_report(y_true, y_pred)}")

    print(f"--- METRÄ°K SÃ–ZLÃœÄÃœ VE YORUMLAR ---")

    print(f"1. RECALL (DuyarlÄ±lÄ±k/Yakalama GÃ¼cÃ¼): {recall:.4f}")
    print(f"   > AnlamÄ±: GerÃ§ekte '{label_name}' olan durumlarÄ±n yÃ¼zde kaÃ§Ä±nÄ± yakalayabildik?")
    print(
        f"   > Yorum: EÄŸer bu sayÄ± dÃ¼ÅŸÃ¼kse (Ã¶rn 0.40), tehlikeyi gÃ¶rÃ¼p uyaramÄ±yoruz demektir. Kritik sistemlerde en Ã¶nemli deÄŸerdir.")
    print(f"   > Hedefimiz: {Config.TARGET_RECALL} (Yani olaylarÄ±n %{int(Config.TARGET_RECALL * 100)}'ini kaÃ§Ä±rmamak).")

    print(f"\n2. PRECISION (Kesinlik/GÃ¼venilirlik): {precision:.4f}")
    print(f"   > AnlamÄ±: Model 'Alarm! {label_name} olacak' dediÄŸinde, ne kadarÄ±nda haklÄ± Ã§Ä±ktÄ±?")
    print(f"   > Yorum: EÄŸer bu sayÄ± dÃ¼ÅŸÃ¼kse, model Ã§ok fazla 'YalancÄ± Ã‡oban' (False Alarm) durumuna dÃ¼ÅŸÃ¼yor demektir.")

    print(f"\n3. F1-SCORE (Denge Skoru): {f1:.4f}")
    print(f"   > AnlamÄ±: Precision ve Recall'un harmonik ortalamasÄ±dÄ±r. (Ä°kisini de dengeleyen tek bir not).")
    print(f"   > Yorum: Modelin genel baÅŸarÄ±sÄ±nÄ± tek sayÄ±yla Ã¶zetler.")

    print(f"\n4. SUPPORT (Destek/Ã–rnek SayÄ±sÄ±): {support_positive} Adet")
    print(f"   > AnlamÄ±: Test verisi iÃ§inde gerÃ§ekten {label_name} olan kaÃ§ adet satÄ±r vardÄ±.")
    print(
        f"   > Yorum: EÄŸer bu sayÄ± Ã§ok azsa (Ã¶rn. 5-10), modelin baÅŸarÄ±sÄ± ÅŸans eseri olabilir. Ä°statistiksel gÃ¼ven iÃ§in Ã¶nemlidir.")

    print(f"\n5. ROC-AUC Skoru: {auc:.4f}")
    print(
        f"   > AnlamÄ±: Modelin 0 ve 1 sÄ±nÄ±flarÄ±nÄ± birbirinden ayÄ±rma yeteneÄŸi. 0.5 yazÄ±-tura (kÃ¶tÃ¼), 1.0 mÃ¼kemmel tahmindir.")
    print(f"{'=' * 65}\n")


# =====================================================
# FEATURE ENGINEERING (BEST PRACTICE - DATA LEAKAGE FIX)
# =====================================================
def calculate_rsi(series, period=14):
    """Suyun deÄŸiÅŸim momentumunu (hÄ±zÄ±nÄ±) Ã¶lÃ§er."""
    delta = series.diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
    rs = gain / loss
    rsi = 100 - (100 / (1 + rs))
    return rsi.fillna(50)


def prepare_data(csv_path):
    print("\n[DATA] Veri yÃ¼kleniyor ve Ã¶zellikler tÃ¼retiliyor...")

    # Dosya kontrolÃ¼ ve Dummy Data (EÄŸer dosya yoksa)
    if not os.path.exists(csv_path):
        print(f"[UYARI] {csv_path} bulunamadÄ±. Dummy (Rastgele) veri Ã¼retiliyor...")
        dates = pd.date_range(start="2023-01-01", periods=1000, freq="D")
        df = pd.DataFrame({
            "Tarih": dates,
            "NoktaAdi": ["Point_A"] * 500 + ["Point_B"] * 500,
            "HealthFactor": np.random.uniform(0, 10, 1000)
        })
    else:
        df = pd.read_csv(csv_path)

    df["Tarih"] = pd.to_datetime(df["Tarih"])
    df = df.sort_values(["NoktaAdi", "Tarih"]).reset_index(drop=True)

    # 1. Temel Delta ve Hedefler
    df["Next_Val"] = df.groupby("NoktaAdi")[Config.TARGET_COL].shift(-1)
    df["Future_Delta"] = df["Next_Val"] - df[Config.TARGET_COL]

    # Binary Hedefler
    df["Target_Drop"] = (df["Future_Delta"] < Config.DROP_THRESHOLD).astype(int)
    df["Target_Rise"] = (df["Future_Delta"] > Config.RISE_THRESHOLD).astype(int)

    # 2. GeÃ§miÅŸ Ã–zellikler (Lags)
    grp = df.groupby("NoktaAdi")[Config.TARGET_COL]
    df["Lag_1"] = grp.shift(1)  # DÃ¼n
    df["Lag_2"] = grp.shift(2)  # Ã–nceki gÃ¼n
    df["Lag_3"] = grp.shift(3)  # 3 gÃ¼n Ã¶nce

    # 3. Ä°statistiksel Ã–zellikler (Rolling) - shift(1) ile Data Leakage Ã¶nlenir
    df["Mean_3D"] = grp.shift(1).rolling(3).mean()
    df["Std_3D"] = grp.shift(1).rolling(3).std()
    df["Mean_7D"] = grp.shift(1).rolling(7).mean()

    # 4. Momentum (RSI) - shift(1) Ã¶nemli
    df["RSI_7"] = df.groupby("NoktaAdi")[Config.TARGET_COL].apply(
        lambda x: calculate_rsi(x.shift(1), 7)
    ).reset_index(0, drop=True)

    # 5. DÃ¶ngÃ¼sel Zaman (Cyclical Features)
    df["Month_Sin"] = np.sin(2 * np.pi * df["Tarih"].dt.month / 12)
    df["Month_Cos"] = np.cos(2 * np.pi * df["Tarih"].dt.month / 12)

    # Temizlik (NaN dÃ¼ÅŸÃ¼r)
    features = [
        "Lag_1", "Lag_2", "Lag_3",
        "Mean_3D", "Std_3D", "Mean_7D",
        "RSI_7", "Month_Sin", "Month_Cos"
    ]

    df_clean = df.dropna(subset=features + ["Target_Drop", "Target_Rise"]).reset_index(drop=True)

    print(f"[DATA] {len(df_clean)} satÄ±r hazÄ±rlandÄ±. Ã–zellik sayÄ±sÄ±: {len(features)}")
    return df_clean, features


# =====================================================
# MODEL COMPETITION ENGINE (XGBOOST EKLENDÄ°)
# =====================================================
def get_models():
    """
    YarÄ±ÅŸtÄ±rÄ±lacak TÃ¼m Modelleri DÃ¶ndÃ¼rÃ¼r.
    Buraya XGBoost, KNN, SVM, RF eklenmiÅŸtir.
    """
    models = {
        # 1. KNN: Basit, mesafe temelli
        "KNN": KNeighborsClassifier(n_neighbors=5),

        # 2. SVM: KarmaÅŸÄ±k sÄ±nÄ±rlarÄ± Ã§izmekte ustadÄ±r
        "SVM": SVC(kernel='rbf', probability=True, random_state=Config.RANDOM_STATE),

        # 3. Random Forest: Klasik, gÃ¼Ã§lÃ¼, ensemble model
        "Random Forest": RandomForestClassifier(n_estimators=200, max_depth=10, n_jobs=-1,
                                                random_state=Config.RANDOM_STATE),

        # 4. XGBoost: Kaggle ÅŸampiyonlarÄ±nÄ±n favorisi (HÄ±zlÄ± ve gÃ¼Ã§lÃ¼)
        "XGBoost": XGBClassifier(n_estimators=200, learning_rate=0.05, n_jobs=-1,
                                 random_state=Config.RANDOM_STATE, eval_metric='logloss'),

        # 5. Gradient Boosting (Sklearn versiyonu - kÄ±yas iÃ§in)
        "Gradient Boosting": GradientBoostingClassifier(n_estimators=200, learning_rate=0.05,
                                                        random_state=Config.RANDOM_STATE),

        # 6. Extra Trees: RF'ye benzer ama daha rastgele (bazen daha iyi geneller)
        "Extra Trees": ExtraTreesClassifier(n_estimators=200, max_depth=10, n_jobs=-1, random_state=Config.RANDOM_STATE)
    }
    return models


def optimize_threshold(y_true, y_probs):
    """Recall >= Hedef olduÄŸu noktada en iyi Precision'Ä± veren eÅŸiÄŸi bulur."""
    precisions, recalls, thresholds = precision_recall_curve(y_true, y_probs)

    # Hedef Recall'u geÃ§en tÃ¼m noktalarÄ± bul
    valid_mask = recalls >= Config.TARGET_RECALL
    if not np.any(valid_mask):
        return 0.5  # Hedef tutmazsa standart eÅŸik

    valid_indices = np.where(valid_mask)[0]
    valid_indices = valid_indices[valid_indices < len(thresholds)]

    if len(valid_indices) == 0:
        return 0.5

    best_idx = valid_indices[np.argmax(precisions[valid_indices])]
    return thresholds[best_idx]


def run_model_competition(X_train_full, y_train_full, task_name="Task"):
    """
    Verilen X ve y Ã¼zerinde 6 modeli CV ile yarÄ±ÅŸtÄ±rÄ±r.
    En iyi modeli, en iyi eÅŸik deÄŸeriyle birlikte dÃ¶ndÃ¼rÃ¼r.
    """
    print(f"\n{'=' * 60}")
    print(f"ğŸ† [VALIDATION PHASE] MODEL YARIÅMASI BAÅLIYOR: {task_name}")
    print(f"   > YarÄ±ÅŸmacÄ±lar: KNN, SVM, Random Forest, XGBoost, GB, Extra Trees")
    print(f"{'=' * 60}")

    models = get_models()
    results = []

    tscv = TimeSeriesSplit(n_splits=3)

    for name, model in models.items():
        fold_f1_scores = []
        fold_thresholds = []

        print(f"   -> {name:18} eÄŸitiliyor...", end=" ")

        for train_idx, val_idx in tscv.split(X_train_full):
            # Split
            X_t, X_v = X_train_full.iloc[train_idx], X_train_full.iloc[val_idx]
            y_t, y_v = y_train_full.iloc[train_idx], y_train_full.iloc[val_idx]

            # Scaling (SVM ve KNN iÃ§in kritik)
            scaler = StandardScaler()
            X_t_scaled = scaler.fit_transform(X_t)
            X_v_scaled = scaler.transform(X_v)

            # SMOTE (Imbalance Fix)
            try:
                smote = SMOTE(random_state=Config.RANDOM_STATE)
                X_t_bal, y_t_bal = smote.fit_resample(X_t_scaled, y_t)
            except:
                X_t_bal, y_t_bal = X_t_scaled, y_t

            # EÄŸitim
            model.fit(X_t_bal, y_t_bal)

            # OlasÄ±lÄ±klar
            probs = model.predict_proba(X_v_scaled)[:, 1]

            # Threshold Optimizasyonu
            best_thresh = optimize_threshold(y_v, probs)
            fold_thresholds.append(best_thresh)

            # Skorlama
            preds = (probs >= best_thresh).astype(int)
            fold_f1_scores.append(f1_score(y_v, preds, zero_division=0))

        avg_f1 = np.mean(fold_f1_scores)
        avg_thresh = np.mean(fold_thresholds)

        print(f"| Ort. F1: {avg_f1:.4f} | EÅŸik: {avg_thresh:.4f}")

        results.append({
            "name": name,
            "model": model,
            "score": avg_f1,
            "threshold": avg_thresh
        })

    # Åampiyonu SeÃ§
    best_result = max(results, key=lambda x: x["score"])
    print(f"\nğŸŒŸ [SONUÃ‡] KAZANAN MODEL ({task_name}): {best_result['name']}")
    print(f"   > Sebebi: Validation setlerinde en yÃ¼ksek F1 Skorunu ({best_result['score']:.4f}) verdi.")

    # Åampiyonu TÃœM Train verisiyle tekrar eÄŸit (Final Model)
    final_model = best_result["model"]
    final_scaler = StandardScaler()
    X_final_scaled = final_scaler.fit_transform(X_train_full)

    try:
        smote_final = SMOTE(random_state=Config.RANDOM_STATE)
        X_bal, y_bal = smote_final.fit_resample(X_final_scaled, y_train_full)
    except:
        X_bal, y_bal = X_final_scaled, y_train_full

    final_model.fit(X_bal, y_bal)

    return final_model, final_scaler, best_result["threshold"], best_result["name"]


# =====================================================
# MAIN PIPELINE
# =====================================================
def run_advanced_pipeline():
    os.makedirs(Config.OUTPUT_DIR, exist_ok=True)

    # 1. Veri HazÄ±rlÄ±ÄŸÄ±
    df, features = prepare_data(Config.CSV_PATH)

    # Train / Test Split
    split_idx = int(len(df) * 0.85)

    X_train = df.iloc[:split_idx][features]
    X_test = df.iloc[split_idx:][features]

    y_train_drop = df.iloc[:split_idx]["Target_Drop"]
    y_test_drop = df.iloc[split_idx:]["Target_Drop"]

    y_train_rise = df.iloc[:split_idx]["Target_Rise"]
    y_test_rise = df.iloc[split_idx:]["Target_Rise"]

    print(f"\n[SPLIT BÄ°LGÄ°SÄ°]")
    print(f"  > Train Set (EÄŸitim): {len(X_train)} satÄ±r.")
    print(f"  > Test Set (SÄ±nav): {len(X_test)} satÄ±r.")

    # 2. DROP Modeli Ä°Ã§in YarÄ±ÅŸma ve SeÃ§im
    drop_model, drop_scaler, drop_thresh, drop_name = run_model_competition(X_train, y_train_drop, "DÃœÅÃœÅ (DROP)")

    # 3. RISE Modeli Ä°Ã§in YarÄ±ÅŸma ve SeÃ§im
    rise_model, rise_scaler, rise_thresh, rise_name = run_model_competition(X_train, y_train_rise, "YÃœKSELÄ°Å (RISE)")

    # 4. Test Setinde Final DeÄŸerlendirme
    print("\n" + "=" * 60)
    print("ğŸš€ [TEST PHASE] TEST SETÄ° FÄ°NAL DEÄERLENDÄ°RMESÄ°")
    print("   > ArtÄ±k modelleri hiÃ§ gÃ¶rmedikleri verilerle sÄ±nÄ±yoruz.")
    print("=" * 60)

    # Drop Tahminleri
    X_test_drop_scaled = drop_scaler.transform(X_test)
    drop_probs = drop_model.predict_proba(X_test_drop_scaled)[:, 1]
    drop_preds = (drop_probs >= drop_thresh).astype(int)

    # Rise Tahminleri
    X_test_rise_scaled = rise_scaler.transform(X_test)
    rise_probs = rise_model.predict_proba(X_test_rise_scaled)[:, 1]
    rise_preds = (rise_probs >= rise_thresh).astype(int)

    # --- DROP Raporu ---
    print(f"\n>>> 1. SENARYO: DÃœÅÃœÅ (DROP) ANALÄ°ZÄ° (Åampiyon Model: {drop_name})")
    explain_metrics(y_test_drop, drop_preds, drop_probs, label_name="DÃœÅÃœÅ")

    # Drop Grafik
    plt.figure(figsize=(6, 5))
    cm_drop = confusion_matrix(y_test_drop, drop_preds)
    sns.heatmap(cm_drop, annot=True, fmt='d', cmap='Reds', xticklabels=["Normal", "DÃ¼ÅŸÃ¼ÅŸ"],
                yticklabels=["Normal", "DÃ¼ÅŸÃ¼ÅŸ"])
    plt.title(f"DÃœÅÃœÅ - Confusion Matrix ({drop_name})")
    plt.savefig(f"{Config.OUTPUT_DIR}/best_drop_model_{drop_name}.png")
    plt.close()

    # --- RISE Raporu ---
    print(f"\n>>> 2. SENARYO: YÃœKSELÄ°Å (RISE) ANALÄ°ZÄ° (Åampiyon Model: {rise_name})")
    explain_metrics(y_test_rise, rise_preds, rise_probs, label_name="YÃœKSELÄ°Å")

    # Rise Grafik
    plt.figure(figsize=(6, 5))
    cm_rise = confusion_matrix(y_test_rise, rise_preds)
    sns.heatmap(cm_rise, annot=True, fmt='d', cmap='Greens', xticklabels=["Normal", "YÃ¼kseliÅŸ"],
                yticklabels=["Normal", "YÃ¼kseliÅŸ"])
    plt.title(f"YÃœKSELÄ°Å - Confusion Matrix ({rise_name})")
    plt.savefig(f"{Config.OUTPUT_DIR}/best_rise_model_{rise_name}.png")
    plt.close()

    # 5. Feature Importance (AÄŸaÃ§ tabanlÄ±lar iÃ§in)
    if hasattr(drop_model, "feature_importances_"):
        print(f"\n[Ã–NEMLÄ° Ã–ZELLÄ°KLER] {drop_name} (Drop) Modeli Neye BakÄ±yor?")
        imps = pd.Series(drop_model.feature_importances_, index=features).sort_values(ascending=False).head(5)
        print(imps)

    # 6. Kombine Tahminler
    final_labels = []
    for d_pred, r_pred, d_prob, r_prob in zip(drop_preds, rise_preds, drop_probs, rise_probs):
        if d_pred == 1 and r_pred == 0:
            final_labels.append("DÃœÅÃœÅ")
        elif r_pred == 1 and d_pred == 0:
            final_labels.append("YÃœKSELÄ°Å")
        elif d_pred == 1 and r_pred == 1:
            # Ã‡akÄ±ÅŸma durumunda olasÄ±lÄ±ÄŸÄ± yÃ¼ksek olanÄ± seÃ§
            final_labels.append("DÃœÅÃœÅ" if d_prob > r_prob else "YÃœKSELÄ°Å")
        else:
            final_labels.append("SABÄ°T")

    # GerÃ§ek Etiketler
    true_labels = []
    for d, r in zip(y_test_drop, y_test_rise):
        if d == 1:
            true_labels.append("DÃœÅÃœÅ")
        elif r == 1:
            true_labels.append("YÃœKSELÄ°Å")
        else:
            true_labels.append("SABÄ°T")

    # Kombine Matris
    plt.figure(figsize=(8, 6))
    cm_comb = confusion_matrix(true_labels, final_labels, labels=["DÃœÅÃœÅ", "SABÄ°T", "YÃœKSELÄ°Å"])
    sns.heatmap(cm_comb, annot=True, fmt='d', cmap='Blues',
                xticklabels=["DÃœÅÃœÅ", "SABÄ°T", "YÃœKSELÄ°Å"],
                yticklabels=["DÃœÅÃœÅ", "SABÄ°T", "YÃœKSELÄ°Å"])
    plt.title("FÄ°NAL KOMBÄ°NE TAHMÄ°N MATRÄ°SÄ°")
    plt.ylabel("GerÃ§ek Durum")
    plt.xlabel("Model Tahmini")
    plt.tight_layout()
    plt.savefig(f"{Config.OUTPUT_DIR}/final_combined_matrix.png")
    print(f"\n[INFO] TÃ¼m grafikler '{Config.OUTPUT_DIR}' klasÃ¶rÃ¼ne kaydedildi.")


if __name__ == "__main__":
    run_advanced_pipeline()